# Copyright (C) 2025 Global Digital Labs (gdlabs.io)
# SPDX-License-Identifier: LGPL-3.0-or-later
"""Practice session workflow using LangGraph.

This workflow manages an interactive practice session:
1. Generate a question using the agent
2. Wait for student answer (interrupt point)
3. Evaluate the answer
4. Generate feedback
5. Update memory
6. Repeat until target question count reached

The workflow uses checkpointing to persist state between interactions,
allowing the student to take breaks and resume later.

Note on async database operations:
LangGraph's `.ainvoke()` runs workflow nodes in separate threads via
`run_in_executor`, which breaks SQLAlchemy's greenlet context. Therefore,
database READ operations must use sync methods with `asyncio.to_thread()`.
WRITE operations (db.add()) work because they are synchronous.
See: /docs/memory-theory-validation/LANGGRAPH-ASYNC-DB-ISSUE.md
"""

import asyncio
import logging
from datetime import datetime
from typing import TYPE_CHECKING, Any, Literal
from uuid import uuid4

if TYPE_CHECKING:
    from sqlalchemy.ext.asyncio import AsyncSession

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.base import BaseCheckpointSaver

from src.core.agents import AgentFactory, AgentExecutionContext
from src.core.agents.capabilities.question_generation import GeneratedQuestion
from src.core.agents.capabilities.answer_evaluation import (
    AnswerEvaluationCapability,
    AnswerEvaluationResult,
)
from src.core.agents.capabilities.feedback_generation import GeneratedFeedback
from src.models.practice import QuestionType
from src.core.memory.manager import MemoryManager
from src.core.memory.rag.retriever import RAGRetriever
from src.core.educational.orchestrator import TheoryOrchestrator
from src.core.emotional import EmotionalStateService
from src.core.personas.manager import PersonaManager
from src.core.orchestration.states.practice import (
    PracticeState,
    QuestionRecord,
    create_initial_practice_state,
)
from src.domains.analytics.events import EventTracker

logger = logging.getLogger(__name__)


class PracticeWorkflow:
    """LangGraph workflow for practice sessions.

    This workflow orchestrates a practice session where a student
    answers questions generated by the AI tutor. The workflow:
    - Adapts question difficulty based on performance
    - Provides personalized feedback
    - Updates memory for future personalization
    - Supports interruption and resumption

    Attributes:
        agent_factory: Factory for creating agents.
        memory_manager: Manager for memory operations.
        theory_orchestrator: Orchestrator for educational theories.
        persona_manager: Manager for personas.

    Example:
        >>> workflow = PracticeWorkflow(agent_factory, memory_manager, ...)
        >>> initial_state = create_initial_practice_state(...)
        >>> result = await workflow.run(initial_state, thread_id="session_123")
    """

    def __init__(
        self,
        agent_factory: AgentFactory,
        memory_manager: MemoryManager,
        rag_retriever: RAGRetriever,
        theory_orchestrator: TheoryOrchestrator,
        persona_manager: PersonaManager,
        checkpointer: BaseCheckpointSaver | None = None,
        emotional_service: EmotionalStateService | None = None,
        db_session: "AsyncSession | None" = None,
    ):
        """Initialize the practice workflow.

        Args:
            agent_factory: Factory for creating agents.
            memory_manager: Manager for memory operations.
            rag_retriever: RAG retriever for content retrieval.
            theory_orchestrator: Orchestrator for educational theories.
            persona_manager: Manager for personas.
            checkpointer: Optional checkpointer for state persistence.
            emotional_service: Service for emotional signal recording.
            db_session: Database session for memory operations within workflow nodes.
                If provided, all memory layer operations will use this session,
                avoiding greenlet errors in async context.
        """
        self._agent_factory = agent_factory
        self._memory_manager = memory_manager
        self._rag_retriever = rag_retriever
        self._theory_orchestrator = theory_orchestrator
        self._persona_manager = persona_manager
        self._checkpointer = checkpointer
        self._emotional_service = emotional_service
        self._db_session = db_session

        # Build the workflow graph
        self._graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """Build the LangGraph state graph.

        Returns:
            Compiled StateGraph for practice sessions.
        """
        # Create the graph with PracticeState
        graph = StateGraph(PracticeState)

        # Add nodes
        graph.add_node("initialize", self._initialize)
        graph.add_node("load_context", self._load_context)
        graph.add_node("generate_question", self._generate_question)
        graph.add_node("wait_for_answer", self._wait_for_answer)
        graph.add_node("evaluate_answer", self._evaluate_answer)
        graph.add_node("generate_feedback", self._generate_feedback)
        graph.add_node("update_memory", self._update_memory)
        graph.add_node("check_completion", self._check_completion)
        graph.add_node("refresh_recommendations", self._refresh_recommendations)
        graph.add_node("complete_session", self._complete_session)
        graph.add_node("handle_error", self._handle_error)

        # Set entry point
        graph.set_entry_point("initialize")

        # Add edges
        graph.add_edge("initialize", "load_context")
        graph.add_edge("load_context", "generate_question")
        graph.add_edge("generate_question", "wait_for_answer")
        graph.add_edge("wait_for_answer", "evaluate_answer")
        graph.add_edge("evaluate_answer", "generate_feedback")
        graph.add_edge("generate_feedback", "update_memory")
        graph.add_edge("update_memory", "check_completion")

        # Conditional edge from check_completion
        # When continuing, refresh theory recommendations before generating next question
        graph.add_conditional_edges(
            "check_completion",
            self._should_continue,
            {
                "continue": "refresh_recommendations",
                "complete": "complete_session",
            },
        )

        # After refreshing recommendations, generate the next question
        graph.add_edge("refresh_recommendations", "generate_question")

        graph.add_edge("complete_session", END)
        graph.add_edge("handle_error", END)

        return graph

    def compile(self) -> Any:
        """Compile the workflow graph.

        Returns:
            Compiled workflow that can be executed.
        """
        return self._graph.compile(
            checkpointer=self._checkpointer,
            interrupt_before=["wait_for_answer"],
        )

    async def run(
        self,
        initial_state: PracticeState,
        thread_id: str,
    ) -> PracticeState:
        """Run the workflow from initial state.

        Args:
            initial_state: Starting state for the workflow.
            thread_id: Thread ID for checkpointing.

        Returns:
            Final workflow state.
        """
        compiled = self.compile()
        config = {"configurable": {"thread_id": thread_id}}

        result = await compiled.ainvoke(initial_state, config=config)
        return result

    async def resume(
        self,
        thread_id: str,
        student_answer: str,
    ) -> PracticeState:
        """Resume workflow with student answer.

        Args:
            thread_id: Thread ID to resume.
            student_answer: Student's answer to current question.

        Returns:
            Updated workflow state.
        """
        compiled = self.compile()
        config = {"configurable": {"thread_id": thread_id}}

        # Update state with student answer using aupdate_state
        await compiled.aupdate_state(
            config,
            {
                "awaiting_answer": False,
                "_pending_answer": student_answer,
            },
        )

        # Resume workflow by passing None (continues from checkpoint)
        result = await compiled.ainvoke(None, config=config)
        return result

    async def submit_answer(
        self,
        thread_id: str,
        answer: str,
        time_spent: int | None = None,
        hints_used: int = 0,
    ) -> PracticeState:
        """Submit an answer and continue workflow.

        This is the primary method for answer submission. It:
        1. Gets current state from checkpoint
        2. Updates state with answer using aupdate_state
        3. Resumes workflow execution with ainvoke(None)
        4. Returns updated state with evaluation

        Args:
            thread_id: Thread ID for checkpointing.
            answer: Student's answer.
            time_spent: Time spent in seconds.
            hints_used: Number of hints used.

        Returns:
            Updated workflow state with evaluation results.
        """
        compiled = self.compile()
        config = {"configurable": {"thread_id": thread_id}}

        # Get current state to verify workflow is interrupted
        state_snapshot = await compiled.aget_state(config)
        if not state_snapshot or not state_snapshot.values:
            raise ValueError(f"No state found for thread {thread_id}")

        # Update state with answer data using aupdate_state
        await compiled.aupdate_state(
            config,
            {
                "awaiting_answer": False,
                "_pending_answer": answer,
                "_pending_time_spent": time_spent,
                "_pending_hints_used": hints_used,
            },
        )

        # Resume workflow by passing None (continues from checkpoint)
        result = await compiled.ainvoke(None, config=config)

        # Add last_evaluation for service compatibility
        # Use _last_evaluation from state (set by _evaluate_answer node)
        evaluation = result.get("_last_evaluation")
        if evaluation:
            result["last_evaluation"] = {
                "is_correct": getattr(evaluation, "is_correct", False),
                "score": getattr(evaluation, "score", 0.0),
                "feedback": getattr(evaluation, "feedback", ""),
                "correct_answer": getattr(evaluation, "correct_answer", None),
                "explanation": getattr(evaluation, "explanation", None),
                "confidence": getattr(evaluation, "confidence", None),
                "evaluation_strategy": getattr(evaluation, "evaluation_strategy", None),
            }

        return result

    # =========================================================================
    # Node Implementations
    # =========================================================================

    async def _initialize(self, state: PracticeState) -> dict:
        """Initialize the practice session.

        Args:
            state: Current workflow state.

        Returns:
            State updates.
        """
        logger.info(
            "Initializing practice session: session=%s, topic=%s",
            state.get("session_id"),
            state.get("topic"),
        )

        return {
            "status": "active",
            "current_question_index": 0,
            "questions": [],
        }

    async def _load_context(self, state: PracticeState) -> dict:
        """Load memory, emotional, and theory context.

        Context can be:
        1. Pre-loaded by the service layer (for transaction isolation) and passed
           in the initial state - in this case, we skip database queries.
        2. Loaded dynamically in this node from all 4 memory layers, emotional
           service, and theory orchestrator.

        Pre-loading is preferred as it prevents SQLAlchemy greenlet errors when
        async database operations run inside LangGraph workflow nodes.

        When coming from Learning Tutor (handoff_context.source == "learning_tutor"):
        - Uses initial_difficulty from handoff for easier start
        - Passes learning context to theory recommendations

        When coming from Companion (handoff_context.source == "companion"):
        - Uses emotional_state from handoff context

        Args:
            state: Current workflow state.

        Returns:
            State updates with context.
        """
        tenant_code = state["tenant_code"]
        student_id = state["student_id"]
        topic = state["topic"]

        # Check if context was pre-loaded by the service layer
        pre_loaded_memory = state.get("memory_context", {})
        pre_loaded_theory = state.get("theory_recommendations", {})

        if pre_loaded_memory and pre_loaded_theory:
            # Context was pre-loaded - skip database queries
            logger.info(
                "Using pre-loaded context: student=%s, topic=%s",
                student_id,
                topic,
            )

            # Apply handoff-based adjustments to pre-loaded theory
            handoff = state.get("handoff_context")
            if handoff and handoff.get("source") == "learning_tutor":
                initial_diff = state.get("initial_difficulty")
                if initial_diff is not None:
                    pre_loaded_theory["difficulty"] = min(
                        pre_loaded_theory.get("difficulty", 0.5),
                        initial_diff,
                    )
                    logger.debug(
                        "Adjusted difficulty for Learning Tutor handoff: %.2f",
                        pre_loaded_theory["difficulty"],
                    )

            # Context is already in state, return empty dict (no updates needed)
            return {}

        # Context not pre-loaded - load dynamically using sync methods
        # LangGraph runs nodes in thread executors which breaks async greenlet context
        # We use asyncio.to_thread with sync database methods to avoid greenlet errors
        logger.debug(
            "Loading context dynamically (sync): student=%s, topic=%s, from_learning_tutor=%s",
            student_id,
            topic,
            state.get("from_learning_tutor", False),
        )

        try:
            from uuid import UUID

            # Convert student_id to UUID for MemoryManager
            student_uuid = UUID(student_id) if isinstance(student_id, str) else student_id

            # Load memory context using sync method via asyncio.to_thread
            # This avoids greenlet errors in LangGraph workflow nodes
            memory_context = await asyncio.to_thread(
                self._memory_manager.get_full_context_sync,
                tenant_code,
                student_uuid,
                topic,  # topic parameter for specific mastery loading
            )

            # Load emotional context - prefer handoff context if available
            emotional_context = None
            handoff = state.get("handoff_context")

            if handoff and handoff.get("source") == "companion":
                # Use emotional state from companion handoff
                emotional_context = handoff.get("emotional_state")
            elif self._emotional_service:
                try:
                    emotional_context = await self._emotional_service.get_current_state(
                        student_id=student_uuid,
                    )
                except Exception as e:
                    logger.debug("Emotional context not available: %s", str(e))

            # Get theory recommendations (with emotional context for adjustment)
            theory_recs = await self._theory_orchestrator.get_recommendations(
                tenant_code=tenant_code,
                student_id=student_id,
                topic=topic,
                memory_context=memory_context,
                emotional_context=emotional_context,
            )

            updates: dict[str, Any] = {
                "memory_context": memory_context.model_dump() if memory_context else {},
                "theory_recommendations": theory_recs.model_dump() if theory_recs else {},
            }

            # Apply handoff-based adjustments
            if handoff and handoff.get("source") == "learning_tutor":
                # If coming from Learning Tutor, adjust initial difficulty
                initial_diff = state.get("initial_difficulty")
                if initial_diff is not None and theory_recs:
                    # Override ZPD difficulty with gentler start
                    theory_dict = updates["theory_recommendations"]
                    theory_dict["difficulty"] = min(
                        theory_dict.get("difficulty", 0.5),
                        initial_diff,
                    )
                    logger.debug(
                        "Adjusted difficulty for Learning Tutor handoff: %.2f",
                        theory_dict["difficulty"],
                    )

            return updates

        except Exception as e:
            logger.warning("Failed to load context: %s", str(e))
            return {
                "memory_context": {},
                "theory_recommendations": {},
            }

    async def _generate_question(self, state: PracticeState) -> dict:
        """Generate a practice question.

        Uses the full theory recommendations to personalize question generation:
        - ZPD difficulty (calculated in _calculate_next_difficulty)
        - Bloom's cognitive level
        - VARK content format
        - Scaffolding level
        - Socratic questioning style

        For RANDOM mode, selects a random topic from available_topics,
        avoiding the last used topic for variety.

        Handles special cases:
        - _offer_retry: Returns the retry question after Practice Helper
        - priority_concepts: Prioritizes weak concepts from Learning Tutor

        The agent receives full memory context for personalization.

        Args:
            state: Current workflow state.

        Returns:
            State updates with generated question.
        """
        # Check for retry after Practice Helper
        if state.get("_offer_retry") and state.get("_retry_question_id"):
            retry_id = state["_retry_question_id"]
            logger.info(
                "Offering retry for question %s after Practice Helper",
                retry_id,
            )

            # Find the retry question in questions list
            for q_record in state.get("questions", []):
                if q_record.get("question_id") == retry_id:
                    return {
                        "current_question": q_record.get("question"),
                        "awaiting_answer": True,
                        "_offer_retry": False,
                        "_retry_question_id": None,
                        "error": None,
                    }

            # If not found, continue with normal generation
            logger.warning("Retry question %s not found, generating new", retry_id)

        logger.info(
            "Generating question %d/%d",
            state["current_question_index"] + 1,
            state["target_question_count"],
        )

        # For RANDOM mode, select a topic from available_topics
        topic_updates = {}
        if state.get("mode") == "random" and state.get("available_topics"):
            selected_topic = self._select_random_topic(state)
            if selected_topic:
                topic_updates = {
                    "topic_full_code": selected_topic.get("full_code"),
                    "topic": selected_topic.get("name", "Mixed Topics"),
                    "last_topic_full_code": selected_topic.get("full_code"),
                    "unit_name": selected_topic.get("unit_name"),
                }
                logger.info(
                    "RANDOM mode: Selected topic '%s' for question %d",
                    selected_topic.get("name"),
                    state["current_question_index"] + 1,
                )

        try:
            # Get agent
            agent = self._agent_factory.get("tutor")

            # Get persona
            persona_id = state.get("persona_id", "tutor")
            persona = None
            try:
                persona = self._persona_manager.get_persona(persona_id)
                agent.set_persona(persona)
            except Exception as e:
                logger.warning("Failed to load persona '%s': %s", persona_id, str(e))

            # Calculate difficulty using ZPD theory
            difficulty = self._calculate_next_difficulty(state)

            # Get full theory recommendations
            theory_recs = state.get("theory_recommendations", {})

            # Get mode config for hint availability
            mode_config = state.get("mode_config", {})
            hints_enabled = mode_config.get("hints_enabled", True)

            # Build memory context for agent
            # Convert dict back to FullMemoryContext if needed
            memory_context = state.get("memory_context", {})
            memory_obj = None
            if memory_context:
                try:
                    from src.models.memory import FullMemoryContext
                    memory_obj = FullMemoryContext.model_validate(memory_context)
                except Exception as e:
                    logger.warning(
                        "Failed to convert memory_context to FullMemoryContext: %s",
                        str(e),
                    )

            # Build theory recommendations object if needed
            theory_obj = None
            if theory_recs:
                try:
                    from src.core.educational.orchestrator import CombinedRecommendation
                    theory_obj = CombinedRecommendation.model_validate(theory_recs)
                except Exception as e:
                    logger.warning(
                        "Failed to convert theory_recommendations to CombinedRecommendation: %s",
                        str(e),
                    )

            # Use selected topic for RANDOM mode, otherwise use state topic
            current_topic = topic_updates.get("topic", state["topic"])
            current_unit = topic_updates.get("unit_name", state.get("unit_name"))

            # Retrieve relevant content via RAG for question generation
            rag_results = []
            try:
                from uuid import UUID
                tenant_code = state["tenant_code"]
                student_uuid = UUID(state["student_id"]) if isinstance(state["student_id"], str) else state["student_id"]

                rag_results = await self._rag_retriever.retrieve(
                    tenant_code=tenant_code,
                    student_id=student_uuid,
                    query=current_topic,
                    sources=["curriculum", "episodic", "associative"],
                    limit=5,
                    min_score=0.5,
                )

                if rag_results:
                    logger.debug(
                        "RAG retrieved %d results for topic '%s'",
                        len(rag_results),
                        current_topic,
                    )
            except Exception as e:
                logger.warning("RAG retrieval failed: %s", str(e))

            # Collect previously asked questions to avoid duplicates
            previous_questions = []
            for q_record in state.get("questions", []):
                q = q_record.get("question")
                if q:
                    if hasattr(q, "content"):
                        previous_questions.append(q.content)
                    elif isinstance(q, dict):
                        previous_questions.append(q.get("content", ""))

            # Get priority concepts from handoff (weak concepts from Learning Tutor)
            priority_concepts = state.get("priority_concepts", [])

            # Build params dict with full educational context
            gen_params = {
                "topic": current_topic,
                "topic_full_code": topic_updates.get("topic_full_code") or state.get("topic_full_code"),
                "difficulty": difficulty,
                # All 7 theory recommendations
                "bloom_level": theory_recs.get("bloom_level", "understand"),
                "content_format": theory_recs.get("content_format", "multimodal"),
                "scaffold_level": theory_recs.get("scaffold_level", "moderate"),
                "questioning_style": theory_recs.get("questioning_style", "guided"),
                "guide_vs_tell_ratio": theory_recs.get("guide_vs_tell_ratio", 0.7),
                "hints_enabled": hints_enabled and theory_recs.get("hints_enabled", True),
                "include_hints": hints_enabled,
                # Mode-specific params
                "mode": state.get("mode", "quick"),
                # Priority concepts from Learning Tutor handoff
                "priority_concepts": priority_concepts,
                "from_learning_tutor": state.get("from_learning_tutor", False),
                # Educational context from curriculum hierarchy
                # These are critical for age-appropriate content generation
                "subject_name": state.get("subject_name"),
                "subject_full_code": state.get("subject_full_code"),
                "grade_level": state.get("grade_level"),
                "grade_level_code": state.get("grade_level_code"),
                "age_range": state.get("age_range"),
                "curriculum": state.get("curriculum"),
                "curriculum_code": state.get("curriculum_code"),
                "language": state.get("language", "en"),
                "unit_name": current_unit,
                # Learning objective for focused question generation
                "learning_objective": state.get("learning_objective_text"),
                "learning_objective_full_code": state.get("learning_objective_full_code"),
                # Previous questions to avoid duplicates
                "previous_questions": previous_questions,
            }

            if priority_concepts:
                logger.debug(
                    "Priority concepts for question generation: %s",
                    priority_concepts[:3],
                )

            # Add question_type if specified in state
            question_type = state.get("question_type")
            if question_type:
                gen_params["question_type"] = question_type

            # Create execution context with full personalization data
            context = AgentExecutionContext(
                tenant_id=state["tenant_id"],
                student_id=state["student_id"],
                topic=current_topic,
                intent="question_generation",
                params=gen_params,
                # Pass rich context for deep personalization
                memory=memory_obj,
                theory=theory_obj,
                rag_results=rag_results,
                persona=persona,
            )

            logger.debug(
                "Question generation context: difficulty=%.2f, bloom=%s, format=%s, scaffold=%s, "
                "grade=%s, subject=%s, language=%s, rag_count=%d",
                difficulty,
                theory_recs.get("bloom_level", "understand"),
                theory_recs.get("content_format", "multimodal"),
                theory_recs.get("scaffold_level", "moderate"),
                state.get("grade_level", "unknown"),
                state.get("subject_name", "unknown"),
                state.get("language", "en"),
                len(rag_results),
            )

            # Build runtime context for YAML-driven system prompt
            runtime_context = self._build_runtime_context(
                state=state,
                theory_recs=theory_recs,
                intent="question_generation",
            )

            # Execute with runtime context
            response = await agent.execute(context, runtime_context=runtime_context)

            if response.success and response.result:
                question = response.result
                question_id = str(uuid4())

                # Create question record
                question_record = QuestionRecord(
                    question_id=question_id,
                    question=question,
                    student_answer=None,
                    evaluation=None,
                    feedback=None,
                    time_taken_seconds=None,
                    hints_used=0,
                )

                questions = list(state.get("questions", []))
                questions.append(question_record)

                # Build return dict with topic updates for RANDOM mode
                result = {
                    "current_question": question,
                    "questions": questions,
                    "awaiting_answer": True,
                    "error": None,
                }
                # Include topic updates if in RANDOM mode
                if topic_updates:
                    result.update(topic_updates)
                return result
            else:
                return {
                    "error": "Failed to generate question",
                    "status": "error",
                }

        except Exception as e:
            logger.exception("Question generation failed")
            return {
                "error": str(e),
                "status": "error",
            }

    async def _wait_for_answer(self, state: PracticeState) -> dict:
        """Wait for student answer (interrupt point).

        This node doesn't do anything - the workflow will be interrupted
        here and resumed when the student provides an answer.

        Args:
            state: Current workflow state.

        Returns:
            State updates.
        """
        logger.debug("Waiting for student answer")

        # This is the interrupt point
        # The workflow will pause here until resumed with an answer
        return {
            "awaiting_answer": True,
        }

    async def _evaluate_answer(self, state: PracticeState) -> dict:
        """Evaluate the student's answer.

        Args:
            state: Current workflow state.

        Returns:
            State updates with evaluation.
        """
        # Get the pending answer (set during resume)
        student_answer = state.get("_pending_answer", "")
        current_question = state.get("current_question")

        if not current_question:
            return {"error": "No current question to evaluate"}

        logger.info("Evaluating answer for question %d", state["current_question_index"] + 1)

        try:
            # Handle both object and dict access for current_question
            if hasattr(current_question, "content"):
                # It's a GeneratedQuestion object
                q_content = current_question.content
                q_correct_answer = current_question.correct_answer
                q_type = current_question.question_type
                q_options = current_question.options
            else:
                # It's a dict
                q_content = current_question.get("content", "")
                q_correct_answer = current_question.get("correct_answer", "")
                q_type = current_question.get("question_type", "short_answer")
                q_options = current_question.get("options")

            # Convert question_type to QuestionType enum if string
            if isinstance(q_type, str):
                try:
                    q_type = QuestionType(q_type)
                except ValueError:
                    q_type = QuestionType.SHORT_ANSWER

            # Build evaluation params
            eval_params = {
                "question_content": q_content,
                "student_answer": student_answer,
                "expected_answer": q_correct_answer,
                "question_type": q_type,
                "topic": state.get("topic", ""),
            }

            # Add options for multiple choice questions
            if q_options:
                # Convert options to list of dicts for the capability
                options_list = []
                for opt in q_options:
                    if hasattr(opt, "model_dump"):
                        options_list.append(opt.model_dump())
                    elif isinstance(opt, dict):
                        options_list.append(opt)
                    else:
                        options_list.append({
                            "key": getattr(opt, "key", ""),
                            "text": getattr(opt, "text", ""),
                            "is_correct": getattr(opt, "is_correct", False),
                        })
                eval_params["options"] = options_list

            # Use hybrid evaluation
            capability = AnswerEvaluationCapability()

            if not capability.needs_llm(eval_params):
                # Rule-based evaluation (MCQ, T/F, numerical, etc.)
                logger.debug("Using rule-based evaluation for %s", q_type)
                evaluation = capability.evaluate_direct(eval_params)
                response_success = True
            else:
                # LLM-based evaluation (semantic, rubric)
                logger.info("Using LLM-based evaluation for %s", q_type)
                agent = self._agent_factory.get("assessor")

                # Set persona for assessor
                persona_id = state.get("persona_id", "tutor")
                try:
                    persona = self._persona_manager.get_persona(persona_id)
                    agent.set_persona(persona)
                except Exception:
                    pass

                context = AgentExecutionContext(
                    tenant_id=state["tenant_id"],
                    student_id=state["student_id"],
                    topic=state["topic"],
                    intent="answer_evaluation",
                    params=eval_params,
                )

                # Build runtime context for YAML-driven system prompt
                theory_recs = state.get("theory_recommendations", {})
                runtime_context = self._build_runtime_context(
                    state=state,
                    theory_recs=theory_recs,
                    intent="answer_evaluation",
                )

                response = await agent.execute(context, runtime_context=runtime_context)
                response_success = response.success and response.result
                evaluation = response.result if response_success else None

            if response_success and evaluation:
                # Update question record
                questions = list(state.get("questions", []))
                if questions:
                    questions[-1]["student_answer"] = student_answer
                    questions[-1]["evaluation"] = evaluation

                # Update metrics
                metrics = dict(state.get("metrics", {}))
                metrics["questions_answered"] = metrics.get("questions_answered", 0) + 1

                # Track consecutive wrong answers for help escalation
                consecutive_wrong = state.get("consecutive_wrong", 0)

                if evaluation.is_correct:
                    metrics["questions_correct"] = metrics.get("questions_correct", 0) + 1
                    metrics["streak_current"] = metrics.get("streak_current", 0) + 1
                    metrics["streak_max"] = max(
                        metrics.get("streak_max", 0),
                        metrics["streak_current"],
                    )
                    consecutive_wrong = 0  # Reset on correct answer
                elif evaluation.score >= 0.5:
                    metrics["questions_partial"] = metrics.get("questions_partial", 0) + 1
                    metrics["streak_current"] = 0
                    consecutive_wrong = 0  # Reset on partial correct
                else:
                    metrics["questions_incorrect"] = metrics.get("questions_incorrect", 0) + 1
                    metrics["streak_current"] = 0
                    consecutive_wrong += 1  # Increment on wrong answer

                # Calculate accuracy
                total = metrics["questions_answered"]
                correct = metrics["questions_correct"]
                partial = metrics["questions_partial"]
                metrics["accuracy"] = (correct + 0.5 * partial) / total if total > 0 else 0.0

                # Generate help options for the response
                help_options = self._get_help_options(state, evaluation)

                return {
                    "questions": questions,
                    "metrics": metrics,
                    "consecutive_wrong": consecutive_wrong,
                    "awaiting_answer": False,
                    "_pending_answer": None,
                    "_last_evaluation": evaluation,  # Store for service compatibility
                    "_help_options": help_options,  # Available help actions
                    "error": None,
                }
            else:
                return {"error": "Failed to evaluate answer"}

        except Exception as e:
            logger.exception("Answer evaluation failed")
            return {"error": str(e)}

    async def _generate_feedback(self, state: PracticeState) -> dict:
        """Generate personalized feedback.

        Args:
            state: Current workflow state.

        Returns:
            State updates with feedback.
        """
        questions = state.get("questions", [])
        if not questions:
            return {}

        current_question_record = questions[-1]
        evaluation = current_question_record.get("evaluation")

        if not evaluation:
            return {}

        logger.debug("Generating feedback")

        try:
            # Get agent
            agent = self._agent_factory.get("tutor")

            # Set persona
            persona_id = state.get("persona_id", "tutor")
            try:
                persona = self._persona_manager.get_persona(persona_id)
                agent.set_persona(persona)
            except Exception:
                pass

            # Determine feedback type based on performance
            metrics = state.get("metrics", {})
            feedback_type = self._determine_feedback_type(
                is_correct=evaluation.is_correct,
                metrics=metrics,
            )

            # Create execution context
            context = AgentExecutionContext(
                tenant_id=state["tenant_id"],
                student_id=state["student_id"],
                topic=state["topic"],
                intent="feedback_generation",
                params={
                    "feedback_type": feedback_type,
                    "is_correct": evaluation.is_correct,
                    "score": evaluation.score,
                    "performance": {
                        "questions_total": metrics.get("questions_answered", 0),
                        "questions_correct": metrics.get("questions_correct", 0),
                        "score": metrics.get("accuracy", 0.0),
                        "streak_current": metrics.get("streak_current", 0),
                    },
                },
            )

            # Build runtime context for YAML-driven system prompt
            theory_recs = state.get("theory_recommendations", {})
            runtime_context = self._build_runtime_context(
                state=state,
                theory_recs=theory_recs,
                intent="feedback_generation",
            )

            # Execute with runtime context
            response = await agent.execute(context, runtime_context=runtime_context)

            if response.success and response.result:
                feedback = response.result

                # Update question record
                questions[-1]["feedback"] = feedback

                return {"questions": questions}

        except Exception as e:
            logger.warning("Feedback generation failed: %s", str(e))

        return {}

    async def _update_memory(self, state: PracticeState) -> dict:
        """Update memory with session progress.

        Updates all 4 memory layers after each answer:
        1. Episodic - Records the learning event
        2. Semantic - Updates topic mastery
        3. Procedural - Records learning patterns (time, format, etc.)
        4. Spaced Repetition - Schedules next review via FSRS

        Args:
            state: Current workflow state.

        Returns:
            State updates.
        """
        questions = state.get("questions", [])
        if not questions:
            return {}

        current = questions[-1]
        evaluation = current.get("evaluation")

        if not evaluation:
            return {}

        logger.debug("Updating memory (4 layers)")

        try:
            tenant_code = state["tenant_code"]

            # 1. Record in episodic memory with cross-workflow context
            topic_full_code = state.get("topic_full_code")
            from_learning_tutor = state.get("from_learning_tutor", False)
            learning_session_id = state.get("learning_session_id")

            # Build event data with handoff context
            # Include hints_used for hint dependency tracking in theory calculations
            event_data = {
                "question_id": current.get("question_id"),
                "is_correct": evaluation.is_correct,
                "score": evaluation.score,
                "session_id": state["session_id"],
                "from_learning_tutor": from_learning_tutor,
                "hints_used": current.get("hints_used", 0),
                "time_spent_seconds": current.get("time_taken_seconds", 0),
            }

            # Add Learning Tutor session ID if available for cross-session linking
            if learning_session_id:
                event_data["learning_session_id"] = learning_session_id

            # Add helper interventions if any
            helper_interventions = state.get("_helper_interventions", [])
            if helper_interventions:
                event_data["helper_interventions"] = helper_interventions

            await self._memory_manager.record_learning_event(
                tenant_code=tenant_code,
                student_id=state["student_id"],
                event_type="question_answered",
                topic=state["topic"],
                data=event_data,
                topic_full_code=topic_full_code,
                session=self._db_session,
            )

            # 2. Update semantic memory (mastery) with proper attempt tracking
            if topic_full_code:
                time_spent = current.get("time_taken_seconds", 0) or 0
                await self._memory_manager.record_practice_attempt(
                    tenant_code=tenant_code,
                    student_id=state["student_id"],
                    topic_full_code=topic_full_code,
                    is_correct=evaluation.is_correct,
                    time_seconds=time_spent,
                    session=self._db_session,
                )

            # 3. Schedule spaced repetition review (FSRS algorithm)
            await self._schedule_spaced_repetition(
                state=state,
                evaluation=evaluation,
                current_question=current,
            )

            # 4. Record procedural observation (learning patterns)
            await self._record_procedural_observation(
                state=state,
                current_question=current,
                evaluation=evaluation,
            )

            # 5. Publish performance event for analytics and diagnostic triggers
            # This is fire-and-forget - errors don't affect main flow
            await self._publish_performance_event(state, evaluation)

        except Exception as e:
            logger.warning("Memory update failed: %s", str(e))

        return {}

    async def _check_completion(self, state: PracticeState) -> dict:
        """Check if practice session should complete.

        Args:
            state: Current workflow state.

        Returns:
            State updates with incremented question index.
        """
        return {
            "current_question_index": state["current_question_index"] + 1,
        }

    def _should_continue(self, state: PracticeState) -> Literal["continue", "complete"]:
        """Determine if workflow should continue or complete.

        Args:
            state: Current workflow state.

        Returns:
            "continue" to generate another question, "complete" to finish.
        """
        current_index = state.get("current_question_index", 0)
        target_count = state.get("target_question_count", 5)

        if current_index >= target_count:
            return "complete"

        # Check for error
        if state.get("status") == "error":
            return "complete"

        return "continue"

    async def _refresh_recommendations(self, state: PracticeState) -> dict:
        """Refresh theory recommendations after each answer.

        This node runs before generating the next question to ensure
        difficulty and other parameters reflect the updated mastery level.
        After a student answers a question, their mastery changes, and
        the ZPD-based difficulty should adapt accordingly.

        The flow is:
        1. Get updated memory context (with new mastery from _update_memory)
        2. Get fresh theory recommendations using updated mastery
        3. Update state with new recommendations for _generate_question

        Args:
            state: Current workflow state.

        Returns:
            State updates with refreshed memory_context and theory_recommendations.
        """
        tenant_code = state["tenant_code"]
        student_id = state["student_id"]
        topic_full_code = state.get("topic_full_code")
        topic_name = state.get("topic")

        logger.debug(
            "Refreshing theory recommendations: student=%s, topic=%s, question_index=%d",
            student_id,
            topic_full_code,
            state.get("current_question_index", 0),
        )

        try:
            from uuid import UUID

            # Convert student_id to UUID for MemoryManager
            student_uuid = UUID(student_id) if isinstance(student_id, str) else student_id

            # Get updated memory context with current mastery using sync method
            # Use topic_full_code for correct mastery lookup from semantic_memories
            # We use asyncio.to_thread to avoid greenlet errors in LangGraph nodes
            memory_context = await asyncio.to_thread(
                self._memory_manager.get_full_context_sync,
                tenant_code,
                student_uuid,
                topic_full_code,  # topic parameter for specific mastery loading
            )

            # Get emotional context if available
            # Note: Emotional service is async but typically does not use DB directly
            emotional_context = None
            if self._emotional_service:
                try:
                    emotional_context = await self._emotional_service.get_current_state(
                        student_id=student_uuid,
                    )
                except Exception as e:
                    logger.debug("Emotional context not available: %s", str(e))

            # Get fresh theory recommendations with updated mastery
            # TheoryOrchestrator.get_recommendations is async but uses memory_context,
            # not direct DB access, so it should work fine
            theory_recs = await self._theory_orchestrator.get_recommendations(
                tenant_code=tenant_code,
                student_id=student_id,
                topic=topic_name,  # topic_name for logging, actual mastery from memory_context
                memory_context=memory_context,
                emotional_context=emotional_context,
            )

            updates = {
                "memory_context": memory_context.model_dump() if memory_context else {},
                "theory_recommendations": theory_recs.model_dump() if theory_recs else {},
            }

            # Log the updated difficulty for tracking
            if theory_recs:
                # Get topic mastery from memory context
                topic_mastery_val = 0.0
                if memory_context and memory_context.topic_mastery:
                    topic_mastery_val = memory_context.topic_mastery.mastery_level
                elif memory_context and memory_context.semantic:
                    topic_mastery_val = memory_context.semantic.overall_mastery

                logger.info(
                    "Theory recommendations refreshed: difficulty=%.3f, mastery=%.3f, bloom=%s",
                    theory_recs.difficulty,
                    topic_mastery_val,
                    theory_recs.bloom_level.value if theory_recs.bloom_level else "unknown",
                )

            return updates

        except Exception as e:
            logger.warning("Failed to refresh recommendations: %s", str(e))
            # Return empty updates to continue with existing recommendations
            return {}

    async def _complete_session(self, state: PracticeState) -> dict:
        """Complete the practice session.

        Args:
            state: Current workflow state.

        Returns:
            State updates marking completion.
        """
        logger.info(
            "Practice session completed: session=%s, questions=%d, accuracy=%.1f%%",
            state.get("session_id"),
            state["metrics"]["questions_answered"],
            state["metrics"]["accuracy"] * 100,
        )

        # Publish session completed event for analytics and diagnostic triggers
        # This is fire-and-forget - errors don't affect main flow
        await self._publish_session_completed_event(state)

        return {
            "status": "completed",
            "completed_at": datetime.now().isoformat(),
            "current_question": None,
            "awaiting_answer": False,
        }

    async def _handle_error(self, state: PracticeState) -> dict:
        """Handle workflow errors.

        Args:
            state: Current workflow state.

        Returns:
            State updates for error state.
        """
        logger.error("Practice workflow error: %s", state.get("error"))

        return {
            "status": "error",
            "completed_at": datetime.now().isoformat(),
        }

    # =========================================================================
    # Helper Methods
    # =========================================================================

    def _select_random_topic(self, state: PracticeState) -> dict[str, Any] | None:
        """Select a random topic from available_topics for RANDOM mode.

        Implements smart selection to avoid consecutive same topic:
        1. If only one topic, return it
        2. If last_topic_full_code exists, exclude it from candidates
        3. Select randomly from remaining candidates

        Args:
            state: Current workflow state with available_topics and last_topic_full_code.

        Returns:
            Selected topic dict with full_code, name, unit_name, or None if no topics.
        """
        import random

        available = state.get("available_topics", [])
        if not available:
            return None

        if len(available) == 1:
            return available[0]

        # Get last topic to avoid repetition
        last_topic_full_code = state.get("last_topic_full_code")

        # Filter out last topic if we have more than one option
        if last_topic_full_code:
            candidates = [t for t in available if t.get("full_code") != last_topic_full_code]
            # If somehow all topics were last topic, fall back to all
            if not candidates:
                candidates = available
        else:
            candidates = available

        # Select randomly
        return random.choice(candidates)

    def _build_runtime_context(
        self,
        state: PracticeState,
        theory_recs: dict[str, Any],
        intent: str,
    ) -> dict[str, Any]:
        """Build runtime context for YAML-driven system prompt building.

        This context is passed to DynamicAgent.execute() for SystemPromptBuilder
        to interpolate variables in the YAML system_prompt configuration.

        Args:
            state: Current workflow state.
            theory_recs: Theory recommendations from TheoryOrchestrator.
            intent: Capability intent (e.g., "question_generation").

        Returns:
            Runtime context dictionary with all variables for prompt interpolation.
        """
        # Get memory context for mastery info
        memory_context = state.get("memory_context", {})

        # Extract mastery level from topic_mastery (SemanticMemoryResponse dict) or overall mastery
        topic_mastery = memory_context.get("topic_mastery")
        semantic = memory_context.get("semantic", {})

        if topic_mastery and isinstance(topic_mastery, dict):
            # topic_mastery is a SemanticMemoryResponse dict with mastery_level field
            mastery_level = topic_mastery.get("mastery_level", 0.5)
        elif semantic and isinstance(semantic, dict):
            # Fall back to overall mastery from semantic overview
            mastery_level = semantic.get("overall_mastery", 0.5)
        else:
            # Default for new students
            mastery_level = 0.5

        # Get learning style from procedural memory
        procedural = memory_context.get("procedural", {})
        learning_style = procedural.get("preferred_content_format", "multimodal")

        # Get student interests from associative memory
        associative = memory_context.get("associative", {})
        interests = associative.get("interests", [])
        if isinstance(interests, list):
            # Each interest item is a dict with "content" field
            interest_names = [
                i.get("content", str(i)) if isinstance(i, dict) else str(i)
                for i in interests
            ]
            interests_str = ", ".join(interest_names) if interest_names else "various topics"
        else:
            interests_str = str(interests) if interests else "various topics"

        # Get metrics for feedback context
        metrics = state.get("metrics", {})

        # Determine feedback tone based on performance
        accuracy = metrics.get("accuracy", 0.5)
        if accuracy >= 0.8:
            feedback_tone = "celebratory and encouraging"
        elif accuracy >= 0.6:
            feedback_tone = "positive and supportive"
        elif accuracy >= 0.4:
            feedback_tone = "constructive and helpful"
        else:
            feedback_tone = "supportive and reassuring"

        # Collect previously asked questions for duplicate avoidance
        previous_questions = []
        for q_record in state.get("questions", []):
            q = q_record.get("question")
            if q:
                if hasattr(q, "content"):
                    previous_questions.append(q.content)
                elif isinstance(q, dict):
                    previous_questions.append(q.get("content", ""))

        # Format previous questions for prompt
        if previous_questions:
            previous_questions_str = "\n".join(f"- {q}" for q in previous_questions)
        else:
            previous_questions_str = "None yet"

        return {
            # Educational context
            "grade_level": state.get("grade_level", 5),
            "age_range": state.get("age_range", "10-11"),
            "subject_name": state.get("subject_name", "General"),
            "topic_name": state.get("topic", "General"),
            "curriculum": state.get("curriculum", "General"),
            "language": state.get("language", "en"),
            # Student context
            "learning_style": learning_style,
            "mastery_level": f"{int(mastery_level * 100)}%",
            "interests": interests_str,
            # Theory guidance
            "difficulty_percent": int(theory_recs.get("difficulty", 0.5) * 100),
            "bloom_level": theory_recs.get("bloom_level", "understand"),
            "content_format": theory_recs.get("content_format", "multimodal"),
            "scaffold_level": theory_recs.get("scaffold_level", "moderate"),
            "questioning_style": theory_recs.get("questioning_style", "guided"),
            # Feedback context
            "feedback_tone": feedback_tone,
            # Evaluation context
            "question_type": state.get("question_type", "short_answer"),
            "partial_credit_enabled": state.get("mode_config", {}).get("partial_credit", True),
            "strict_evaluation": state.get("mode_config", {}).get("strict_evaluation", False),
            # Previous questions to avoid duplicates
            "previous_questions": previous_questions_str,
            "has_previous_questions": len(previous_questions) > 0,
            # Condition flags for conditional sections
            "has_grade_level": state.get("grade_level") is not None,
            "has_mastery_level": mastery_level is not None,
            "has_difficulty_percent": theory_recs.get("difficulty") is not None,
            "has_question_type": state.get("question_type") is not None,
            "has_theory": bool(theory_recs),
        }

    def _determine_feedback_type(
        self,
        is_correct: bool,
        metrics: dict,
    ) -> str:
        """Determine appropriate feedback type based on performance.

        Selects the most appropriate feedback type from FeedbackType enum
        based on the student's current answer result and overall metrics.

        Args:
            is_correct: Whether the current answer was correct.
            metrics: Session metrics including streak and accuracy.

        Returns:
            Valid FeedbackType enum value as string.
        """
        streak = metrics.get("streak_current", 0)
        accuracy = metrics.get("accuracy", 0.5)

        if is_correct:
            # Celebrate streaks, otherwise encourage
            if streak >= 3:
                return "milestone_reached"
            return "encouragement"
        else:
            # Provide support if struggling, otherwise update on progress
            if accuracy < 0.4:
                return "struggle_support"
            return "progress_update"

    def _get_help_options(
        self,
        state: PracticeState,
        evaluation: Any,
    ) -> list[dict[str, Any]]:
        """Generate intelligent help options based on context.

        Provides contextual help options after answer evaluation:
        - Always offers hint (if enabled)
        - Offers Practice Helper for wrong answers
        - Offers Learning Tutor for low mastery or consecutive failures

        Args:
            state: Current workflow state.
            evaluation: Answer evaluation result.

        Returns:
            List of help option dictionaries with action, label, description.
        """
        options: list[dict[str, Any]] = []
        mode_config = state.get("mode_config", {})
        hints_enabled = mode_config.get("hints_enabled", True)

        # Always offer hint if enabled
        if hints_enabled:
            options.append({
                "action": "hint",
                "label": "Give me a hint",
                "description": "A small clue to help you figure it out",
            })

        # Offer Practice Helper for wrong answers
        if not evaluation.is_correct:
            options.append({
                "action": "help_understand",
                "label": "Help me understand",
                "description": "Get guided help for this question",
                "triggers_workflow": "practice_helper",
                "handoff_context": {
                    "source": "practice",
                    "practice_session_id": state.get("session_id"),
                    "question": state.get("current_question"),
                    "student_answer": state.get("_pending_answer"),
                    "evaluation_result": (
                        evaluation.model_dump()
                        if hasattr(evaluation, "model_dump")
                        else {}
                    ),
                    "misconceptions": (
                        [m.model_dump() for m in evaluation.misconceptions]
                        if hasattr(evaluation, "misconceptions") and evaluation.misconceptions
                        else []
                    ),
                    "topic_code": state.get("topic_full_code"),
                },
            })

        # Offer Learning Tutor for low mastery or consecutive failures
        memory_context = state.get("memory_context", {})
        semantic = memory_context.get("semantic", {})
        topic_code = state.get("topic_full_code", "")

        # Get topic mastery from semantic memory
        topic_mastery = 0.5
        if topic_code and isinstance(semantic, dict):
            topic_data = semantic.get(topic_code, {})
            if isinstance(topic_data, dict):
                topic_mastery = topic_data.get("mastery", 0.5)

        consecutive_wrong = state.get("consecutive_wrong", 0)

        if topic_mastery < 0.3 or consecutive_wrong >= 3:
            options.append({
                "action": "learn_topic",
                "label": "I need to learn this topic",
                "description": "Get a full lesson on this topic",
                "triggers_workflow": "learning_tutor",
                "handoff_context": {
                    "source": "practice",
                    "practice_session_id": state.get("session_id"),
                    "topic_code": state.get("topic_full_code"),
                    "topic_mastery": topic_mastery,
                    "consecutive_wrong": consecutive_wrong,
                    "misconceptions": (
                        [m.model_dump() for m in evaluation.misconceptions]
                        if hasattr(evaluation, "misconceptions") and evaluation.misconceptions
                        else []
                    ),
                },
            })

        return options

    def _calculate_next_difficulty(self, state: PracticeState) -> float:
        """Calculate difficulty using ZPD theory recommendations.

        Uses the ZPD (Zone of Proximal Development) theory to determine
        optimal difficulty based on mastery, performance streaks, and
        diagnostic indicators.

        The workflow loads theory recommendations in _load_context() node,
        where TheoryOrchestrator.get_recommendations() is called. This
        combines all 7 theories including ZPD which calculates:
        - Base mastery from semantic memory
        - ZPD center = mastery + (lower + upper) / 2
        - Frustration reduction when consecutive_incorrect >= threshold
        - Confidence increase when consecutive_correct >= threshold
        - Diagnostic adjustments for learning difficulties

        Falls back to base difficulty if:
        - Mode is not adaptive and mode_config.adaptive_difficulty is False
        - Theory recommendations not available
        - ZPD difficulty not calculated

        Args:
            state: Current workflow state.

        Returns:
            Difficulty value (0.0-1.0).
        """
        logger.debug("_calculate_next_difficulty called, mode=%s", state.get("mode"))

        # Get mode configuration
        mode = state.get("mode", "quick")
        mode_config = state.get("mode_config", {})

        # Check if adaptive difficulty is enabled from mode config
        # All modes have adaptive_difficulty=True by default for personalized learning
        # Only explicitly False disables ZPD-based difficulty adaptation
        adaptive_enabled = mode_config.get("adaptive_difficulty", True)

        if not adaptive_enabled:
            # Use base difficulty for non-adaptive modes
            return state.get("difficulty", 0.5)

        # Get ZPD recommendation from theory recommendations
        theory_recs = state.get("theory_recommendations", {})
        zpd_difficulty = theory_recs.get("difficulty")

        if zpd_difficulty is not None:
            # Apply mode's difficulty range constraints if specified
            diff_range = mode_config.get("difficulty_range")
            if diff_range and len(diff_range) == 2:
                min_diff, max_diff = diff_range
                zpd_difficulty = max(min_diff, min(max_diff, zpd_difficulty))

            logger.debug(
                "ZPD difficulty applied: %.3f (mode=%s, adaptive=%s)",
                zpd_difficulty,
                mode,
                adaptive_enabled,
            )
            return zpd_difficulty

        # Fallback to base difficulty if ZPD not available
        logger.debug(
            "ZPD not available, fallback to base: %.2f (mode=%s)",
            state.get("difficulty", 0.5),
            mode,
        )
        return state.get("difficulty", 0.5)

    async def _schedule_spaced_repetition(
        self,
        state: PracticeState,
        evaluation: Any,
        current_question: dict,
    ) -> None:
        """Schedule next review using FSRS algorithm.

        Retrieves existing FSRS parameters from semantic memory, calculates
        the next review using the FSRS algorithm, and persists the updated
        parameters back to semantic memory.

        The rating is determined from:
        - Whether the answer was correct
        - Response time relative to expected time
        - Whether hints were used

        Args:
            state: Current workflow state.
            evaluation: Answer evaluation result.
            current_question: Current question record.
        """
        import uuid as uuid_module

        from src.core.educational.theories.spaced_repetition import (
            SpacedRepetitionTheory,
        )
        from src.models.memory import EntityType

        try:
            # Get spaced repetition theory instance
            sr_theory = self._theory_orchestrator.get_theory("spaced_repetition")
            if not sr_theory or not isinstance(sr_theory, SpacedRepetitionTheory):
                logger.debug("Spaced repetition theory not available")
                return

            # Get required context
            tenant_code = state.get("tenant_code")
            student_id = state.get("student_id")
            topic_name = state.get("topic")
            topic_full_code = state.get("topic_full_code")

            if not tenant_code or not student_id or not topic_name:
                logger.debug("Missing tenant_code, student_id, or topic for FSRS scheduling")
                return

            if not topic_full_code:
                logger.debug("Missing topic_full_code for FSRS scheduling")
                return

            # Get time spent and hints from question record
            time_spent = current_question.get("time_taken_seconds")
            hints_used = current_question.get("hints_used", 0)

            # Determine FSRS rating from performance
            rating = SpacedRepetitionTheory.rating_from_performance(
                correct=evaluation.is_correct,
                response_time_seconds=time_spent,
                used_hint=hints_used > 0,
                difficulty=state.get("difficulty", 0.5),
            )

            # Get or create semantic memory for this topic
            semantic_layer = self._memory_manager.semantic
            student_uuid = uuid_module.UUID(student_id)

            # Get or create semantic memory for this topic using entity_full_code
            existing_memory = await semantic_layer.get_or_create_by_full_code(
                tenant_code=tenant_code,
                student_id=student_uuid,
                entity_type=EntityType.TOPIC,
                entity_full_code=topic_full_code,
                session=self._db_session,
            )

            # Get card parameters from existing memory or create new
            if existing_memory and existing_memory.fsrs_stability is not None:
                card_params = {
                    "stability": existing_memory.fsrs_stability,
                    "difficulty": existing_memory.fsrs_difficulty,
                    "state": existing_memory.fsrs_state or "learning",
                    "step": existing_memory.fsrs_step or 0,
                    "last_review": existing_memory.fsrs_last_review,
                }
                logger.debug(
                    "Using existing FSRS params: stability=%.4f, state=%s",
                    card_params["stability"],
                    card_params["state"],
                )
            else:
                card_params = sr_theory.create_new_card()
                logger.debug("Creating new FSRS card for topic")

            # Schedule next review
            review_result = sr_theory.schedule_review(
                stability=card_params["stability"],
                difficulty=card_params["difficulty"],
                state=card_params["state"],
                step=card_params["step"],
                last_review=card_params["last_review"],
                rating=rating,
            )

            logger.info(
                "FSRS scheduled: topic=%s, rating=%s, new_state=%s, due=%s",
                state.get("topic"),
                rating.name,
                review_result["state"],
                review_result["due"],
            )

            # Persist FSRS parameters to semantic memory
            if existing_memory:
                await semantic_layer.update_fsrs_parameters(
                    tenant_code=tenant_code,
                    memory_id=existing_memory.id,
                    stability=review_result["stability"],
                    difficulty=review_result["difficulty"],
                    state=review_result["state"],
                    step=review_result["step"],
                    due_at=review_result["due"],
                    last_review=review_result["last_review"],
                    session=self._db_session,
                )
                logger.debug(
                    "FSRS parameters persisted to semantic memory: %s",
                    existing_memory.id,
                )

        except Exception as e:
            logger.warning("Spaced repetition scheduling failed: %s", str(e))

    async def _record_procedural_observation(
        self,
        state: PracticeState,
        current_question: dict,
        evaluation: Any,
    ) -> None:
        """Record learning pattern observation in procedural memory.

        Tracks behavioral patterns that inform personalization:
        - When the student learns best (time of day)
        - How they interact with content (formats, hints)
        - Response patterns (speed, accuracy by type)

        This data feeds into VARK theory calculations and
        helps optimize future content delivery.

        VARK Profile Building:
        - Infers VARK-compatible content format from question characteristics
        - Maps question_type and display_hint to visual/text/practice/audio
        - Records effectiveness based on answer correctness
        - Over time builds personalized learning style profile

        Args:
            state: Current workflow state.
            current_question: Current question record.
            evaluation: Answer evaluation result.
        """
        from datetime import datetime

        try:
            tenant_code = state["tenant_code"]
            student_id = state["student_id"]

            # Determine time of day bucket
            current_hour = datetime.now().hour
            if 5 <= current_hour < 12:
                time_of_day = "morning"
            elif 12 <= current_hour < 17:
                time_of_day = "afternoon"
            elif 17 <= current_hour < 21:
                time_of_day = "evening"
            else:
                time_of_day = "night"

            # Get question data
            question = current_question.get("question", {})
            if hasattr(question, "question_type"):
                question_type = str(question.question_type)
            elif isinstance(question, dict):
                question_type = question.get("question_type", "short_answer")
            else:
                question_type = "short_answer"

            # Get display_hint from question (if available)
            if hasattr(question, "display_hint"):
                display_hint = question.display_hint
            elif isinstance(question, dict):
                display_hint = question.get("display_hint")
            else:
                display_hint = None

            # Infer VARK-compatible content format from question characteristics
            # This enables VARK theory to build actual learning style profiles
            content_format = self._infer_vark_content_format(
                question_type=question_type,
                display_hint=display_hint,
            )

            # Build observation data
            observation = {
                "time_of_day": time_of_day,
                "content_format": content_format,
                "question_type": question_type,
                "response_time_seconds": current_question.get("time_taken_seconds"),
                "hints_used": current_question.get("hints_used", 0),
                "is_correct": evaluation.is_correct,
                "score": evaluation.score,
                "session_id": state["session_id"],
                "topic": state["topic"],
                "mode": state.get("mode", "quick"),
            }

            # Record to procedural memory via MemoryManager
            topic_full_code = state.get("topic_full_code")
            await self._memory_manager.record_procedural_observation(
                tenant_code=tenant_code,
                student_id=student_id,
                observation=observation,
                topic_full_code=topic_full_code,
                session=self._db_session,
            )

            logger.debug(
                "Procedural observation: time=%s, format=%s, type=%s, correct=%s",
                time_of_day,
                content_format,
                question_type,
                evaluation.is_correct,
            )

        except Exception as e:
            logger.warning("Procedural observation recording failed: %s", str(e))

    def _infer_vark_content_format(
        self,
        question_type: str,
        display_hint: str | None,
    ) -> str:
        """Infer VARK-compatible content format from question characteristics.

        Maps question attributes to one of the VARK learning styles to enable
        building personalized learning style profiles over time.

        VARK Mapping:
        - visual: Questions with diagrams, images, charts, videos
        - text: Reading-heavy questions, text-based explanations
        - practice: Interactive, hands-on, calculation, problem-solving
        - audio: Audio-based content (less common in practice)

        The mapping prioritizes display_hint (explicit format) over question_type
        (implicit format inference).

        Args:
            question_type: Type of question (e.g., "multiple_choice", "short_answer").
            display_hint: Optional explicit content format hint from question.

        Returns:
            VARK-compatible format: "visual", "text", "practice", or "audio".
        """
        # Mapping from display_hint values to VARK styles
        display_hint_mapping = {
            # Visual
            "visual": "visual",
            "diagram": "visual",
            "image": "visual",
            "chart": "visual",
            "graph": "visual",
            "video": "visual",
            "picture": "visual",
            "illustration": "visual",
            # Text/Reading
            "text": "text",
            "reading": "text",
            "notes": "text",
            "written": "text",
            "passage": "text",
            # Practice/Kinesthetic
            "interactive": "practice",
            "hands_on": "practice",
            "exercise": "practice",
            "calculation": "practice",
            "problem": "practice",
            "activity": "practice",
            # Audio
            "audio": "audio",
            "lecture": "audio",
            "spoken": "audio",
            "listening": "audio",
        }

        # Mapping from question_type to VARK styles
        question_type_mapping = {
            # Practice/Kinesthetic - requires active engagement
            "calculation": "practice",
            "numerical": "practice",
            "equation": "practice",
            "fill_blank": "practice",
            "ordering": "practice",
            "matching": "practice",
            "drag_drop": "practice",
            "interactive": "practice",
            # Text/Reading - requires reading comprehension
            "short_answer": "text",
            "essay": "text",
            "long_answer": "text",
            "multiple_choice": "text",
            "true_false": "text",
            # Visual - image-based questions
            "image_based": "visual",
            "diagram_label": "visual",
            "graph_reading": "visual",
        }

        # Priority 1: Check display_hint if available
        if display_hint:
            hint_lower = display_hint.lower().replace("-", "_").replace(" ", "_")
            if hint_lower in display_hint_mapping:
                return display_hint_mapping[hint_lower]

        # Priority 2: Infer from question_type
        type_lower = question_type.lower().replace("-", "_").replace(" ", "_")
        if type_lower in question_type_mapping:
            return question_type_mapping[type_lower]

        # Default: text (most common for academic content)
        return "text"

    # =========================================================================
    # Event Publishing Methods (Async, Fire-and-Forget)
    # =========================================================================

    async def _publish_performance_event(
        self,
        state: PracticeState,
        evaluation: Any,
    ) -> None:
        """Publish performance event for analytics and diagnostic triggers.

        This event enables:
        - Analytics tracking via EventBus  Bridge  Dramatiq
        - Diagnostic triggers (on_answer_evaluated) for low accuracy detection

        Fire-and-forget: Errors are logged but don't break the main flow.
        Critical memory updates (semantic, FSRS) remain synchronous.

        Args:
            state: Current workflow state.
            evaluation: Answer evaluation result.
        """
        try:
            tracker = EventTracker(tenant_code=state["tenant_code"])

            # Get question type from current question
            current_question = state.get("current_question")
            question_type = None
            if current_question:
                if hasattr(current_question, "question_type"):
                    question_type = str(current_question.question_type)
                elif isinstance(current_question, dict):
                    question_type = current_question.get("question_type")

            await tracker.track_performance_event(
                student_id=state["student_id"],
                session_id=state["session_id"],
                is_correct=evaluation.is_correct,
                score=evaluation.score,
                question_type=question_type,
                data={
                    "topic": state.get("topic"),
                    "difficulty": state.get("difficulty"),
                    "mode": state.get("mode"),
                },
            )

            logger.debug(
                "Performance event published: session=%s, is_correct=%s",
                state["session_id"],
                evaluation.is_correct,
            )

        except Exception as e:
            # Fire-and-forget: Log but don't fail the main flow
            logger.warning("Failed to publish performance event: %s", str(e))

    async def _publish_session_completed_event(
        self,
        state: PracticeState,
    ) -> None:
        """Publish session completed event for analytics and diagnostic triggers.

        This event enables:
        - Analytics tracking via EventBus  Bridge  Dramatiq
        - Diagnostic triggers (on_session_completed) for threshold checks

        Fire-and-forget: Errors are logged but don't break the main flow.

        Args:
            state: Current workflow state.
        """
        try:
            tracker = EventTracker(tenant_code=state["tenant_code"])

            metrics = state.get("metrics", {})

            await tracker.track_engagement_event(
                event_subtype="completed",
                student_id=state["student_id"],
                session_id=state["session_id"],
                session_type="practice",
                data={
                    "topic": state.get("topic"),
                    "questions_answered": metrics.get("questions_answered", 0),
                    "questions_correct": metrics.get("questions_correct", 0),
                    "accuracy": metrics.get("accuracy", 0.0),
                    "mode": state.get("mode"),
                },
            )

            logger.debug(
                "Session completed event published: session=%s, accuracy=%.1f%%",
                state["session_id"],
                metrics.get("accuracy", 0.0) * 100,
            )

        except Exception as e:
            # Fire-and-forget: Log but don't fail the main flow
            logger.warning("Failed to publish session completed event: %s", str(e))
